{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch.optim as optim\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below I have data that represents time invariant or time variant data. to test 1 set of data simply uncomment the other set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Time invariant data is below\n",
    "'''\n",
    "\n",
    "#file_path = r\"C:\\Users\\steph\\OneDrive\\Desktop\\Stephen\\ADCL\\thesis stuff\\ALL THESIS DATA\\large data PINN model data\\Data for time domain pinn\\theta_cmd_3_pilot_simulator\\invariant_cmd3_cessna\\tlds.csv\"\n",
    "#tlds = torch.tensor((pd.read_csv(file_path)).values)\n",
    "\n",
    "#file_path = r\"C:\\Users\\steph\\OneDrive\\Desktop\\Stephen\\ADCL\\thesis stuff\\ALL THESIS DATA\\large data PINN model data\\Data for time domain pinn\\theta_cmd_3_pilot_simulator\\invariant_cmd3_cessna\\tlgs.csv\"\n",
    "#tlgs = torch.tensor((pd.read_csv(file_path)).values)\n",
    "\n",
    "#file_path = r\"C:\\Users\\steph\\OneDrive\\Desktop\\Stephen\\ADCL\\thesis stuff\\ALL THESIS DATA\\large data PINN model data\\Data for time domain pinn\\theta_cmd_3_pilot_simulator\\invariant_cmd3_cessna\\taus.csv\"\n",
    "#taus = torch.tensor((pd.read_csv(file_path)).values)\n",
    "\n",
    "#file_path = r\"C:\\Users\\steph\\OneDrive\\Desktop\\Stephen\\ADCL\\thesis stuff\\ALL THESIS DATA\\large data PINN model data\\Data for time domain pinn\\theta_cmd_3_pilot_simulator\\invariant_cmd3_cessna\\kps.csv\"\n",
    "#kps = torch.tensor((pd.read_csv(file_path)).values)\n",
    "\n",
    "#file_path = r\"C:\\Users\\steph\\OneDrive\\Desktop\\Stephen\\ADCL\\thesis stuff\\ALL THESIS DATA\\large data PINN model data\\Data for time domain pinn\\theta_cmd_3_pilot_simulator\\invariant_cmd3_cessna\\pilotoutput.csv\"\n",
    "#p = torch.tensor((pd.read_csv(file_path)).values)\n",
    "\n",
    "#file_path = r\"C:\\Users\\steph\\OneDrive\\Desktop\\Stephen\\ADCL\\thesis stuff\\ALL THESIS DATA\\large data PINN model data\\Data for time domain pinn\\theta_cmd_3_pilot_simulator\\invariant_cmd3_cessna\\pitcherror.csv\"\n",
    "#pitch_error = torch.tensor((pd.read_csv(file_path)).values)\n",
    "\n",
    "#file_path = r\"C:\\Users\\steph\\OneDrive\\Desktop\\Stephen\\ADCL\\thesis stuff\\ALL THESIS DATA\\large data PINN model data\\Data for time domain pinn\\theta_cmd_3_pilot_simulator\\invariant_cmd3_cessna\\pitchcmd.csv\"\n",
    "#pitch_cmd = torch.tensor((pd.read_csv(file_path)).values)\n",
    "\n",
    "#file_path = r\"C:\\Users\\steph\\OneDrive\\Desktop\\Stephen\\ADCL\\thesis stuff\\ALL THESIS DATA\\large data PINN model data\\Data for time domain pinn\\theta_cmd_3_pilot_simulator\\invariant_cmd3_cessna\\pitchoutput.csv\"\n",
    "#pitch_out = torch.tensor((pd.read_csv(file_path)).values)\n",
    "\n",
    "#file_path = r\"C:\\Users\\steph\\OneDrive\\Desktop\\Stephen\\ADCL\\thesis stuff\\ALL THESIS DATA\\large data PINN model data\\Data for time domain pinn\\theta_cmd_3_pilot_simulator\\invariant_cmd3_cessna\\xpilot.csv\"\n",
    "#xp = torch.tensor((pd.read_csv(file_path)).values)\n",
    "\n",
    "\n",
    "'''\n",
    "Time variant data below\n",
    "'''\n",
    "file_path = r\"C:\\Users\\steph\\OneDrive\\Desktop\\Stephen\\ADCL\\thesis stuff\\ALL THESIS DATA\\large data PINN model data\\Data for time domain pinn\\theta_cmd_3_pilot_simulator\\variant_cmd3_cessna\\tlds.csv\"\n",
    "tlds = torch.tensor((pd.read_csv(file_path)).values)\n",
    "\n",
    "file_path = r\"C:\\Users\\steph\\OneDrive\\Desktop\\Stephen\\ADCL\\thesis stuff\\ALL THESIS DATA\\large data PINN model data\\Data for time domain pinn\\theta_cmd_3_pilot_simulator\\variant_cmd3_cessna\\tlgs.csv\"\n",
    "tlgs = torch.tensor((pd.read_csv(file_path)).values)\n",
    "\n",
    "file_path = r\"C:\\Users\\steph\\OneDrive\\Desktop\\Stephen\\ADCL\\thesis stuff\\ALL THESIS DATA\\large data PINN model data\\Data for time domain pinn\\theta_cmd_3_pilot_simulator\\variant_cmd3_cessna\\taus.csv\"\n",
    "taus = torch.tensor((pd.read_csv(file_path)).values)\n",
    "\n",
    "file_path = r\"C:\\Users\\steph\\OneDrive\\Desktop\\Stephen\\ADCL\\thesis stuff\\ALL THESIS DATA\\large data PINN model data\\Data for time domain pinn\\theta_cmd_3_pilot_simulator\\variant_cmd3_cessna\\kps.csv\"\n",
    "kps = torch.tensor((pd.read_csv(file_path)).values)\n",
    "\n",
    "file_path = r\"C:\\Users\\steph\\OneDrive\\Desktop\\Stephen\\ADCL\\thesis stuff\\ALL THESIS DATA\\large data PINN model data\\Data for time domain pinn\\theta_cmd_3_pilot_simulator\\variant_cmd3_cessna\\pilotoutput.csv\"\n",
    "p = torch.tensor((pd.read_csv(file_path)).values)\n",
    "\n",
    "file_path = r\"C:\\Users\\steph\\OneDrive\\Desktop\\Stephen\\ADCL\\thesis stuff\\ALL THESIS DATA\\large data PINN model data\\Data for time domain pinn\\theta_cmd_3_pilot_simulator\\variant_cmd3_cessna\\pitcherror.csv\"\n",
    "pitch_error = torch.tensor((pd.read_csv(file_path)).values)\n",
    "\n",
    "file_path = r\"C:\\Users\\steph\\OneDrive\\Desktop\\Stephen\\ADCL\\thesis stuff\\ALL THESIS DATA\\large data PINN model data\\Data for time domain pinn\\theta_cmd_3_pilot_simulator\\variant_cmd3_cessna\\pitchcmd.csv\"\n",
    "pitch_cmd = torch.tensor((pd.read_csv(file_path)).values)\n",
    "\n",
    "file_path = r\"C:\\Users\\steph\\OneDrive\\Desktop\\Stephen\\ADCL\\thesis stuff\\ALL THESIS DATA\\large data PINN model data\\Data for time domain pinn\\theta_cmd_3_pilot_simulator\\variant_cmd3_cessna\\pitchoutput.csv\"\n",
    "pitch_out = torch.tensor((pd.read_csv(file_path)).values)\n",
    "\n",
    "file_path = r\"C:\\Users\\steph\\OneDrive\\Desktop\\Stephen\\ADCL\\thesis stuff\\ALL THESIS DATA\\large data PINN model data\\Data for time domain pinn\\theta_cmd_3_pilot_simulator\\variant_cmd3_cessna\\xpilot.csv\"\n",
    "xp = torch.tensor((pd.read_csv(file_path)).values)\n",
    "\n",
    "Ts = 0.1\n",
    "Tfinal = 100\n",
    "length = Tfinal/Ts\n",
    "time = torch.arange(0, Tfinal + Ts, Ts)[:-1].view(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis/cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are all columns of tlgs larger than corresponding columns of tlds: False\n",
      "Column numbers where max value exceeds the threshold: [98, 132, 137, 145, 227, 235, 272, 288, 371, 384, 604]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 739]),\n",
       " torch.Size([1000, 739]),\n",
       " torch.Size([1000, 739]),\n",
       " torch.Size([1000, 739]),\n",
       " torch.Size([1000, 739]),\n",
       " torch.Size([1000, 739]),\n",
       " torch.Size([1000, 739]),\n",
       " torch.Size([1000, 739]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if each column of tensor1 is larger than the corresponding column of tensor2\n",
    "is_column_larger = tlgs > tlds\n",
    "\n",
    "# Check if all columns of tensor1 are larger than corresponding columns of tensor2\n",
    "are_all_columns_larger = torch.all(is_column_larger, dim=0)\n",
    "\n",
    "print(\"Are all columns of tlgs larger than corresponding columns of tlds:\", torch.all(are_all_columns_larger).item())\n",
    "\n",
    "# Threshold value\n",
    "threshold = 1.7\n",
    "\n",
    "# Find the maximum value of each column\n",
    "max_values, _ = torch.max(pitch_out, dim=0)\n",
    "\n",
    "# Obtain the column numbers where the maximum value exceeds the threshold\n",
    "columns_above_threshold = torch.nonzero(max_values > threshold).squeeze()\n",
    "\n",
    "print(\"Column numbers where max value exceeds the threshold:\", columns_above_threshold.tolist())\n",
    "\n",
    "# Create a mask to select columns to keep\n",
    "mask = torch.ones(pitch_out.size(1), dtype=torch.bool)\n",
    "mask[columns_above_threshold] = False\n",
    "\n",
    "# Delete columns based on the mask\n",
    "pitch_out_filtered = pitch_out[:, mask]\n",
    "\n",
    "\n",
    "tlds_filtered = tlds[:,mask]\n",
    "tlgs_filtered = tlgs[:,mask]\n",
    "taus_filtered = taus[:,mask]\n",
    "kps_filtered=kps[:,mask]\n",
    "p_filtered = p[:,mask]\n",
    "pitch_cmd_filtered = pitch_cmd[:,mask]\n",
    "pitch_error_filtered = pitch_error[:,mask]\n",
    "xp_filtered = xp[:,mask]\n",
    "\n",
    "pitch_out_filtered.shape, tlds_filtered.shape, tlgs_filtered.shape, kps_filtered.shape,p_filtered.shape, pitch_cmd_filtered.shape,pitch_error_filtered.shape,xp_filtered.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert data to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_31832\\2570946064.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tlds_tensor = torch.tensor(tlds_filtered, dtype=torch.float32)\n",
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_31832\\2570946064.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tlgs_tensor = torch.tensor(tlgs_filtered, dtype=torch.float32)\n",
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_31832\\2570946064.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  taus_tensor = torch.tensor(taus_filtered, dtype=torch.float32)\n",
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_31832\\2570946064.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  kps_tensor = torch.tensor(kps_filtered, dtype=torch.float32)\n",
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_31832\\2570946064.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_tensor = torch.tensor(p_filtered, dtype=torch.float32)\n",
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_31832\\2570946064.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pitch_error_tensor = torch.tensor(pitch_error_filtered, dtype=torch.float32)\n",
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_31832\\2570946064.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pitch_cmd_tensor = torch.tensor(pitch_cmd_filtered, dtype=torch.float32)\n",
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_31832\\2570946064.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pitch_out_tensor = torch.tensor(pitch_out_filtered, dtype=torch.float32)\n",
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_31832\\2570946064.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xp_tensor = torch.tensor(xp_filtered, dtype=torch.float32)\n",
      "C:\\Users\\steph\\AppData\\Local\\Temp\\ipykernel_31832\\2570946064.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  time_tensor = torch.tensor(time, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "tlds_tensor = torch.tensor(tlds_filtered, dtype=torch.float32)\n",
    "tlgs_tensor = torch.tensor(tlgs_filtered, dtype=torch.float32)\n",
    "taus_tensor = torch.tensor(taus_filtered, dtype=torch.float32)\n",
    "kps_tensor = torch.tensor(kps_filtered, dtype=torch.float32)\n",
    "p_tensor = torch.tensor(p_filtered, dtype=torch.float32)\n",
    "pitch_error_tensor = torch.tensor(pitch_error_filtered, dtype=torch.float32)\n",
    "pitch_cmd_tensor = torch.tensor(pitch_cmd_filtered, dtype=torch.float32)\n",
    "pitch_out_tensor = torch.tensor(pitch_out_filtered, dtype=torch.float32)\n",
    "xp_tensor = torch.tensor(xp_filtered, dtype=torch.float32)\n",
    "time_tensor = torch.tensor(time, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_cmd_tensor_norm = (pitch_cmd_tensor - pitch_cmd_tensor.min(dim=0).values) / (pitch_cmd_tensor.max(dim=0).values - pitch_cmd_tensor.min(dim=0).values)\n",
    "pitch_out_tensor_norm = (pitch_out_tensor - pitch_out_tensor.min(dim=0).values) / (pitch_out_tensor.max(dim=0).values - pitch_out_tensor.min(dim=0).values)\n",
    "pitch_error_tensor_norm = (pitch_error_tensor - pitch_error_tensor.min(dim=0).values) / (pitch_error_tensor.max(dim=0).values - pitch_error_tensor.min(dim=0).values)\n",
    "pilot_output_tensor_norm = (p_tensor - p_tensor.min(dim=0).values) / (p_tensor.max(dim=0).values - p_tensor.min(dim=0).values)\n",
    "time_norm = (time - time.min(dim=0).values) / (time.max(dim=0).values - time.min(dim=0).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([739, 1000, 5])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_list = []\n",
    "for i in range(tlds_tensor.shape[1]):\n",
    "  tensor_2d = (torch.cat((pitch_cmd_tensor_norm[:length,i].view(-1,1), pitch_out_tensor_norm[:length,i].view(-1,1),pitch_error_tensor_norm[:length,i].view(-1,1),pilot_output_tensor_norm[:length,i].view(-1,1), time_norm[:length]),dim=1))\n",
    "  tensor_list.append(tensor_2d)\n",
    "X = torch.stack(tensor_list)\n",
    "X.shape #this is in the shape of (# of runs, data points, # of features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([739, 5000])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tlds_tensor_2 =  tlds_tensor.T\n",
    "tlgs_tensor_2 = tlgs_tensor.T\n",
    "taus_tensor_2 = taus_tensor.T\n",
    "kps_tensor_2 = kps_tensor.T\n",
    "tlds_tensor_2.shape\n",
    "\n",
    "tensor_list = []\n",
    "for j in range(tlds_tensor.shape[1]):\n",
    "  row_tensor = torch.concat((xp_tensor.T[j,:length].view(1,-1),tlds_tensor_2[j,:length].view(1,-1),tlgs_tensor_2[j,:length].view(1,-1),taus_tensor_2[j,:length].view(1,-1), kps_tensor_2[j,:length].view(1,-1)),dim=1)\n",
    "  tensor_list.append(row_tensor)\n",
    "\n",
    "y = torch.stack(tensor_list)\n",
    "y=torch.squeeze(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([739, 1000]), torch.Size([739, 1000]))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitch_error_tensor.T.shape, p_tensor.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([739, 1000]), torch.Size([739, 1000]))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pitch_error_tensor_reshape = pitch_error_tensor.T\n",
    "p_tensor_reshape = p_tensor.T\n",
    "pitch_error_tensor_reshape.shape, p_tensor_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([591, 1000, 5]),\n",
       " torch.Size([148, 1000, 5]),\n",
       " torch.Size([591, 1000]),\n",
       " torch.Size([148, 1000]),\n",
       " torch.Size([591, 1000]),\n",
       " torch.Size([148, 1000]),\n",
       " torch.Size([591, 5000]),\n",
       " torch.Size([148, 5000]))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, pitcherrortrain, pitcherrortest, ptrain, ptest, ytrain, ytest = train_test_split(\n",
    "    X, pitch_error_tensor_reshape, p_tensor_reshape, y,\n",
    "    test_size=0.2,        # Proportion of the dataset to include in the test split\n",
    "    random_state=123       # Seed for the random number generator\n",
    ")\n",
    "\n",
    "Xtrain.shape, Xtest.shape, pitcherrortrain.shape, pitcherrortest.shape, ptrain.shape, ptest.shape, ytrain.shape, ytest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below is the RNN, LSTM, CNN models. comment out whichever u dont need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model\n",
    "class BasicRNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(BasicRNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define RNN layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, nonlinearity=\"tanh\")\n",
    "\n",
    "        # Define fully connected layers for the output\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)  # Hidden size reduced by half\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  # Output layer\n",
    "\n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  # hidden state\n",
    "\n",
    "        # Forward pass through RNN layer\n",
    "        out, _ = self.rnn(x, h0)\n",
    "\n",
    "        # Take the output from the last time step\n",
    "        out = out[:, -1, :]  # Shape: (batch_size, hidden_size)\n",
    "\n",
    "        # Apply ReLU activation\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # Pass through the first fully connected layer\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # Pass through the second fully connected layer (output layer)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Example Usage:\n",
    "# Define parameters\n",
    "input_size = 5       # Number of input features\n",
    "hidden_size = 64     # Number of hidden units\n",
    "output_size = 5000   # Output dimension\n",
    "num_layers = 1       # Number of RNN layers\n",
    "\n",
    "# Create an instance of the model\n",
    "model = BasicRNNModel(input_size, hidden_size, output_size, num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM model\n",
    "\n",
    "class BasicLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(BasicLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Define fully connected layers for the output\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)  # Hidden size reduced by half\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)  # Output layer\n",
    "\n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  # hidden state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  # cell state\n",
    "\n",
    "        # Forward pass through LSTM layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Take the output from the last time step\n",
    "        out = out[:, -1, :]  # Shape: (batch_size, hidden_size)\n",
    "\n",
    "        # Apply ReLU activation\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # Pass through the first fully connected layer\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # Pass through the second fully connected layer (output layer)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "# Example Usage:\n",
    "# Define parameters\n",
    "input_size = Xtrain.shape[2]   # Number of input features\n",
    "hidden_size = 64  # Number of hidden units\n",
    "output_size = ytrain.shape[1]   # Output dimension (for regression, binary classification, etc.)\n",
    "num_layers = 1    # Number of LSTM layers\n",
    "\n",
    "# Create an instance of the model\n",
    "model = BasicLSTMModel(input_size, hidden_size, output_size, num_layers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model\n",
    "\n",
    "class BasicCNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(BasicCNNModel, self).__init__()\n",
    "        \n",
    "        # Convolutional Layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=hidden_size, kernel_size=10, padding=1)\n",
    "        \n",
    "        # Define a fully connected hidden layer\n",
    "        self.fc_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc_output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (Batch, Seq_len, Features) -> (Batch, Features, Seq_len) for Conv1d\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Apply Convolutional layer\n",
    "        out = self.conv1(x)\n",
    "        \n",
    "        # Apply ReLU activation\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # Apply Global Max Pooling (collapse sequence dimension)\n",
    "        out, _ = torch.max(out, dim=2)\n",
    "        \n",
    "        # Pass through the fully connected hidden layer\n",
    "        out = self.fc_hidden(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # Pass through the output layer\n",
    "        out = self.fc_output(out)\n",
    "        \n",
    "        # Apply sigmoid scaling to specific indices\n",
    "        #out[:, 750:1000] = torch.sigmoid(out[:, 750:1000]) * 0.9 + 0.1  # Scale between 0.1 and 1\n",
    "\n",
    "        return out\n",
    "\n",
    "# Example Usage:\n",
    "# Define parameters\n",
    "input_size = Xtrain.shape[2]   # Number of input features\n",
    "hidden_size = 128  # Number of hidden units (out_channels for CNN)\n",
    "output_size = ytrain.shape[1]   # Output dimension (for regression, binary classification, etc.)\n",
    "num_layers = 1    # (Not needed for CNN, but kept to mirror LSTM)\n",
    "\n",
    "# Create an instance of the CNN model\n",
    "model = BasicCNNModel(input_size, hidden_size, output_size, num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "torch.manual_seed(123)\n",
    "# Create a custom dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X_train, y_train, additional_data1, additional_data2):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.additional_data1 = additional_data1\n",
    "        self.additional_data2 = additional_data2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X_train[idx]\n",
    "        y = self.y_train[idx]\n",
    "        additional1 = self.additional_data1[idx]\n",
    "        additional2 = self.additional_data2[idx]\n",
    "        return x, y, additional1, additional2\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create the custom dataset\n",
    "custom_dataset = CustomDataset(X_train=Xtrain, y_train=ytrain, additional_data1=ptrain, additional_data2=pitcherrortrain)\n",
    "\n",
    "# Create DataLoader with shuffle=False\n",
    "train_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# define the optimiser\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# define the number of epochs\n",
    "epochs = 50\n",
    "\n",
    "# collect the loss history\n",
    "total_loss_history=[]\n",
    "pilot_output_loss_history=[]\n",
    "xpilot_calcualted_loss_history=[]\n",
    "params_loss_history=[]\n",
    "\n",
    "\n",
    "# define the loss criteria\n",
    "criterion= nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def transport_delay(data, tau, time_step, initial_value):\n",
    "    \"\"\"\n",
    "    Applies a transport delay to a 1D tensor of time series data.\n",
    "\n",
    "    Parameters:\n",
    "    - data: 1D PyTorch tensor containing the time series data.\n",
    "    - tau: The delay in seconds.\n",
    "    - time_step: The time step between consecutive samples in seconds.\n",
    "    - initial_value: The value to fill in the delayed portion of the tensor.\n",
    "\n",
    "    Returns:\n",
    "    - delayed_data: The tensor with the applied delay.\n",
    "    \"\"\"\n",
    "    if not isinstance(data, torch.Tensor):\n",
    "        raise ValueError(\"data should be a PyTorch tensor\")\n",
    "\n",
    "    if data.dim() != 1:\n",
    "        raise ValueError(\"data should be a 1D tensor\")\n",
    "\n",
    "    # Calculate the number of samples to delay\n",
    "    #delay_steps = int(round(tau / time_step))\n",
    "    delay_steps = (int((tau / time_step)))\n",
    "\n",
    "    # Ensure delay_steps is non-negative\n",
    "    delay_steps = max(delay_steps, 0)\n",
    "\n",
    "    # Create a tensor with the same shape as data, filled with initial_value\n",
    "    delayed_data = torch.full_like(data, initial_value)\n",
    "\n",
    "    # Apply the delay by shifting the data tensor\n",
    "    if delay_steps < len(data):\n",
    "        delayed_data[delay_steps:] = data[:-delay_steps]\n",
    "    else:\n",
    "        # If delay_steps is larger than the data length, return all initial values\n",
    "        delayed_data = torch.full_like(data, initial_value)\n",
    "\n",
    "    return delayed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for this paper we tried 2 different training loops, one without physics informed (top code), and 1 with physics informed(bottom code). uncomment the 1 u dont need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no physics informed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Total Loss: 0.2490\n",
      "Epoch 2/50, Total Loss: 0.0637\n",
      "Epoch 3/50, Total Loss: 0.0596\n",
      "Epoch 4/50, Total Loss: 0.0572\n",
      "Epoch 5/50, Total Loss: 0.0567\n",
      "Epoch 6/50, Total Loss: 0.0561\n",
      "Epoch 7/50, Total Loss: 0.0559\n",
      "Epoch 8/50, Total Loss: 0.0555\n",
      "Epoch 9/50, Total Loss: 0.0552\n",
      "Epoch 10/50, Total Loss: 0.0547\n",
      "Epoch 11/50, Total Loss: 0.0549\n",
      "Epoch 12/50, Total Loss: 0.0539\n",
      "Epoch 13/50, Total Loss: 0.0524\n",
      "Epoch 14/50, Total Loss: 0.0507\n",
      "Epoch 15/50, Total Loss: 0.0484\n",
      "Epoch 16/50, Total Loss: 0.0454\n",
      "Epoch 17/50, Total Loss: 0.0427\n",
      "Epoch 18/50, Total Loss: 0.0410\n",
      "Epoch 19/50, Total Loss: 0.0392\n",
      "Epoch 20/50, Total Loss: 0.0374\n",
      "Epoch 21/50, Total Loss: 0.0358\n",
      "Epoch 22/50, Total Loss: 0.0342\n",
      "Epoch 23/50, Total Loss: 0.0326\n",
      "Epoch 24/50, Total Loss: 0.0311\n",
      "Epoch 25/50, Total Loss: 0.0300\n",
      "Epoch 26/50, Total Loss: 0.0294\n",
      "Epoch 27/50, Total Loss: 0.0289\n",
      "Epoch 28/50, Total Loss: 0.0283\n",
      "Epoch 29/50, Total Loss: 0.0278\n",
      "Epoch 30/50, Total Loss: 0.0272\n",
      "Epoch 31/50, Total Loss: 0.0267\n",
      "Epoch 32/50, Total Loss: 0.0262\n",
      "Epoch 33/50, Total Loss: 0.0258\n",
      "Epoch 34/50, Total Loss: 0.0252\n",
      "Epoch 35/50, Total Loss: 0.0248\n",
      "Epoch 36/50, Total Loss: 0.0243\n",
      "Epoch 37/50, Total Loss: 0.0238\n",
      "Epoch 38/50, Total Loss: 0.0233\n",
      "Epoch 39/50, Total Loss: 0.0227\n",
      "Epoch 40/50, Total Loss: 0.0221\n",
      "Epoch 41/50, Total Loss: 0.0216\n",
      "Epoch 42/50, Total Loss: 0.0211\n",
      "Epoch 43/50, Total Loss: 0.0207\n",
      "Epoch 44/50, Total Loss: 0.0203\n",
      "Epoch 45/50, Total Loss: 0.0199\n",
      "Epoch 46/50, Total Loss: 0.0195\n",
      "Epoch 47/50, Total Loss: 0.0192\n",
      "Epoch 48/50, Total Loss: 0.0189\n",
      "Epoch 49/50, Total Loss: 0.0186\n",
      "Epoch 50/50, Total Loss: 0.0183\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#scheduler = ReduceLROnPlateau(optimiser, 'min', patience=len(train_loader)*3, factor=0.1)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    tensor_list = []\n",
    "    xpilot_list=[]\n",
    "\n",
    "    for batch_X, batch_y, batch_additional1, batch_additional2 in train_loader:\n",
    "\n",
    "      # Zero the gradients\n",
    "      optimiser.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      xp_est, params_est = model(batch_X)[:,:1000], model(batch_X)[:,1000:]\n",
    "      \n",
    "      # Compute Pilot Parameters Loss\n",
    "      params_loss = criterion(params_est, batch_y[:,1000:])\n",
    "\n",
    "\n",
    "      # Compute X_pilot loss\n",
    "      xpilot_loss = criterion(xp_est[:,:1000], batch_y[:,:1000])\n",
    "\n",
    "      # Compute Total Loss\n",
    "      loss = xpilot_loss + params_loss\n",
    "\n",
    "      total_loss_history.append(loss.item())\n",
    "\n",
    "      # Backward pass\n",
    "      loss.backward()\n",
    "\n",
    "      # Update step\n",
    "      optimiser.step()\n",
    "      \n",
    "    # Print results\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Total Loss: {loss.item():.4f}\")\n",
    "\n",
    "#if epoch >20:\n",
    "#scheduler.step(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with physics informed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "#scheduler = ReduceLROnPlateau(optimiser, 'min', patience=len(train_loader)*10, factor=0.1, verbose=True)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "switch_epoch = 10\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    tensor_list = []\n",
    "    xpilot_list=[]\n",
    "\n",
    "    for batch_X, batch_y, batch_additional1, batch_additional2 in train_loader:\n",
    "\n",
    "      # Zero the gradients\n",
    "      optimiser.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      xp_est, params_est = model(batch_X)[:,:1000], model(batch_X)[:,-4000:]\n",
    "\n",
    "      # Compute X_pilot loss\n",
    "      xpilot_loss = criterion(xp_est[:,:300], batch_y[:,:300])\n",
    "\n",
    "      # Compute parameters loss\n",
    "      #lambda_reg = 0.1\n",
    "      #tld_loss = criterion(params_est[:,0:1000], batch_y[:,1000:2000]) #+ lambda_reg*torch.sum((params_est[:,0:1000]- batch_y[:,1000:2000])**2)\n",
    "      #tlg_loss = criterion(params_est[:,1000:2000],batch_y[:,2000:3000]) #+ lambda_reg*torch.sum((params_est[:,1000:2000]- batch_y[:,2000:3000])**2)\n",
    "      #tau_loss = criterion(params_est[:,2000:3000],batch_y[:,3000:4000])\n",
    "      #kp_loss = criterion(params_est[:,3000:],batch_y[:,4000:])\n",
    "      #params_loss = tld_loss + tlg_loss + tau_loss + kp_loss\n",
    "      params_loss = criterion(params_est, batch_y[:,-4000:])\n",
    "      params_loss_history.append(params_loss.item())\n",
    "\n",
    "      if epoch >= switch_epoch:\n",
    "        # Compute physics losses\n",
    "        for x in range(batch_X.shape[0]):\n",
    "          u = xp_est[x,:]\n",
    "          tld_t,tlg_t,tau_t,kp_t = params_est[x,:1000],params_est[x,1000:2000],params_est[x,2000:3000], params_est[x,3000:4000]\n",
    "          if tau_t.mean() < 0.1:\n",
    "            tau_t_use = 0.1\n",
    "          else:\n",
    "            tau_t_use = tau_t.mean()\n",
    "          pitcherror_t_tau = transport_delay(batch_additional2[x,:], tau_t_use, 0.1, batch_additional2[x,:][0])\n",
    "\n",
    "          # Calculate estimated pilot output\n",
    "          pilot_output_estimated = ((-kp_t/(tlg_t*tlg_t))*(tlg_t-tld_t)*u - (kp_t*tld_t/tlg_t)*pitcherror_t_tau).view(1,-1)\n",
    "          tensor_list.append(pilot_output_estimated)\n",
    "\n",
    "          # Calculate estimated x pilot\n",
    "          xpilot_calcualted = ((pitcherror_t_tau)/tlg_t) - ((pitcherror_t_tau/tlg_t)*(torch.exp(-time.squeeze()/tlg_t))).reshape(1,-1)\n",
    "          xpilot_list.append(xpilot_calcualted)\n",
    "\n",
    "\n",
    "        # Compute pilot output loss\n",
    "        pilot_output_concat = torch.cat(tensor_list, dim=0)\n",
    "        pilot_output_loss = criterion(pilot_output_concat,batch_additional1)\n",
    "        pilot_output_loss_history.append(pilot_output_loss.item())\n",
    "        tensor_list.clear()\n",
    "\n",
    "        # Compute calculated x pilot loss\n",
    "        xpilot_calcualted_concat = torch.cat(xpilot_list,dim=0)\n",
    "        xpilot_calculated_loss = criterion(xpilot_calcualted_concat,batch_y[:,:1000])\n",
    "        xpilot_calcualted_loss_history.append(xpilot_calculated_loss)\n",
    "        xpilot_list.clear()\n",
    "\n",
    "\n",
    "      if epoch < switch_epoch:\n",
    "        loss = xpilot_loss + params_loss\n",
    "      else:\n",
    "        loss = xpilot_loss + params_loss + pilot_output_loss + xpilot_calculated_loss\n",
    "      total_loss_history.append(loss.item())\n",
    "\n",
    "\n",
    "      # Backward pass\n",
    "      loss.backward()\n",
    "\n",
    "\n",
    "      # Update step\n",
    "      optimiser.step()\n",
    "    # Print results\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Total Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Step the scheduler\n",
    "    #scheduler.step(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing and prediction metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.024960136, 0.07758582, 0.0041543623, 0.006745785)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window_size = 1000\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "  # grab estimated params\n",
    "  dense_model_preds = model(Xtest)\n",
    "  estimated_params = dense_model_preds[:,window_size:]\n",
    "  tld_est,tlg_est,tau_est,kp_est = estimated_params[:,:window_size],estimated_params[:,window_size:2*window_size],estimated_params[:,2*window_size:3*window_size],estimated_params[:,3*window_size:4*window_size]\n",
    "\n",
    "\n",
    "  # grab true params\n",
    "  true_params = ytest[:,window_size:]\n",
    "  tld_true,tlg_true,tau_true,kp_true = true_params[:,:window_size],true_params[:,window_size:2*window_size], true_params[:,2*window_size:3*window_size],true_params[:,3*window_size:4*window_size]\n",
    "\n",
    "  from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "  tld_mse = mean_squared_error(tld_true,tld_est)\n",
    "  tlg_mse = mean_squared_error(tlg_true,tlg_est)\n",
    "  tau_mse = mean_squared_error(tau_true,tau_est)\n",
    "  kp_mse = mean_squared_error(kp_true,kp_est)\n",
    "\n",
    "tld_mse, tlg_mse, tau_mse, kp_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004916807"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xp_true = ytest[:,:window_size]\n",
    "estimated_xp = dense_model_preds[:,:window_size]\n",
    "mean_squared_error(xp_true,estimated_xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3651288e-05"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty list to store the tensors\n",
    "pilot_output_predicted_list = []\n",
    "\n",
    "for k in range(len(Xtest)):\n",
    "  tld_est_int = tld_est[k,:]\n",
    "  tlg_est_int = tlg_est[k,:]\n",
    "  tau_est_int = tau_est[k,:]\n",
    "  tau_est_avg = tau_est_int.mean()\n",
    "  kp_est_int = kp_est[k,:]\n",
    "\n",
    "  pitcherror_t_tau = transport_delay(pitcherrortest[k,:], tau_est_avg, 0.1, pitcherrortest[k,:][0])\n",
    "  pilot_output_predicted = (-kp_est_int/(tlg_est_int*tlg_est_int)) * (tlg_est_int-tld_est_int) * estimated_xp[k,:] - (kp_est_int*tld_est_int/tlg_est_int)*pitcherror_t_tau\n",
    "\n",
    "  # Append to the list\n",
    "  pilot_output_predicted_list.append(pilot_output_predicted)\n",
    "\n",
    "\n",
    "# Convert the list to a 2D tensor (NumPy array)\n",
    "pilot_output_predicted_tensor = np.stack(pilot_output_predicted_list)\n",
    "\n",
    "mean_squared_error(ptest,pilot_output_predicted_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'scitechpaper1_cnn_with_physics.pth') # insert appropiate model name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
